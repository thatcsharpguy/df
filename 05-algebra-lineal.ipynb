{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-0",
   "metadata": {},
   "source": [
    "# 05: 츼lgebra Lineal Computacional (Vectores)\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtu.be/bgEML8Re2ks\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-1",
   "metadata": {},
   "source": [
    "## Paquetes  \n",
    "\n",
    " - `numpy`\n",
    " - `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from df.display_algebra import show_vector, show_normed_vects, draw_covariance_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-3",
   "metadata": {},
   "source": [
    "## Espacios vectoriales  \n",
    "\n",
    "Supongamos que tenemos un conjunto de vectores $[x_1, x_2, \\dots x_d], x_i \\in \\mathbb{R}$ (vamos a hablar sobre n칰meros reales, pero podemos generalizar a cualquier otro conjunto de n칰meros) de una dimensi칩n $d$, nosotros podr칤amos decir que este conjunto de vectores existe dentro de un espacio vectorial si se cumplen ciertas condiciones...   \n",
    "\n",
    "Digamos que tenemos los vectores:   \n",
    "\n",
    "$$\\vec{x} = [x_1, x_2, \\dots, x_d]$$  \n",
    "$$\\vec{y} = [y_1, y_2, \\dots, y_d]$$\n",
    "\n",
    "### Operaciones en espacios vectoriales  \n",
    "\n",
    "Entre las condiciones que deben cumplirse est치n que debemos tener bien definidas dos operaciones:  \n",
    "\n",
    " - **multiplicaci칩n por un escalar**: de tal modo que $\\alpha \\vec{x}$ est치 definido para cualquier n칰mero escalar $\\alpha$. En nuestro caso de los reales, $\\alpha \\vec{x} = [\\alpha x_1, \\alpha x_2, \\dots, \\alpha x_d]$  \n",
    " \n",
    " - **suma de vectores**: de tal modo que $\\vec{x} + \\vec{y}$, En nuestro caso de los reales, $\\vec{x} + \\vec{y} = [x_1 + y_1, x_2 + y_2, \\dots x_d + y_d]$\n",
    " \n",
    "<small>Existen 10 axiomas basadas en estas dos operaciones que deben cumplirse.</small>\n",
    "\n",
    "\n",
    "Existe un subconjunto de espacios vectoriales, llamado **espacio vectorial topol칩gico** bajo los cuales debemos definir otro par de operaciones:    \n",
    "\n",
    " - **norma**: $||\\vec{x}||$ que define la longitud de un vector, 칰til para medir y comparar vectores \n",
    "\n",
    " - **producto interno**: $\\langle \\vec{x} | \\vec{y} \\rangle$ or $\\vec{x} \\bullet \\vec{y}$  which allows the angles of two vectors to be compared. The inner product of two orthogonal vectors is 0. For real vectors $\\vec{x} \\bullet \\vec{y} = x_1 y_1 + x_2 y_2 + x_3 y_3 \\dots x_d y_d$\n",
    "  \n",
    "Las operaciones que definimos son importantes para la ciencia de datos porque nos permite hablar de que un par de vectores est치n cerca el uno del otro, nos permite sumarlos, compararlos y escalarlos.\n",
    "\n",
    "<img src=\"assets/05/vectores.png\" />\n",
    "\n",
    "### Recordando *NumPy*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 1])\n",
    "y = np.array([4, 5, 6])\n",
    "a = 2\n",
    "\n",
    "\n",
    "def pp(text, value):\n",
    "    print(text.rjust(10), \"=\", str(value))\n",
    "\n",
    "\n",
    "pp(\"a\", a)\n",
    "pp(\"x\", x)\n",
    "pp(\"y\", y)\n",
    "pp(\"a * x\", a * x)\n",
    "pp(\"x + y\", x + y)\n",
    "pp(\"||x||\", np.linalg.norm(x))\n",
    "pp(\"x 췅 y\", np.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-5",
   "metadata": {},
   "source": [
    "- **`np.linalg.norm`**  \n",
    "- **`np.dot`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-6",
   "metadata": {},
   "source": [
    "### Informaci칩n como vectores  \n",
    "\n",
    "Las representaciones vectoriales son de vital importancia en el aprendizaje autom치tico, puesto que la informaci칩n es m치s f치cilmente procesada por un algoritmo cuando es representada en forma de vectores.\n",
    "\n",
    "Pero antes de hablar de vectores, es importante entender que muchas veces la informaci칩n no est치 en forma vectorial por naturaleza, en muchos casos el proceso es el siguiente:  \n",
    "\n",
    "<img src=\"assets/05/experiment.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-7",
   "metadata": {},
   "source": [
    "### Un cl치sico de machine learning...   \n",
    "\n",
    "<img src=\"assets/05/flower.png\" />\n",
    "\n",
    "<small>Imagen original en el curso de Data Fundamentals <a href=\"https://www.gla.ac.uk/undergraduate/degrees/computingscience/?card=course&code=COMPSCI4073\">https://www.gla.ac.uk/undergraduate/degrees/computingscience/?card=course&code=COMPSCI4073</a></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-8",
   "metadata": {},
   "source": [
    "## Operaciones vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-9",
   "metadata": {},
   "source": [
    "### Sumas y multiplicaciones  \n",
    "\n",
    "Ya hablamos de las sumas y multiplicaciones *elemento-elemento*; una consecuencia de esto es que podemos introducir el concepto de sumas ponderadas:   \n",
    "\n",
    "$$\\lambda_1 x_1 + \\lambda_2 x_2 + \\dots + \\lambda_n x_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 1.0])\n",
    "b = np.array([0.0, 0.5])\n",
    "c = np.array([0.5, 0.0])\n",
    "\n",
    "origin = np.array([0, 0])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5), dpi=200)\n",
    "ax = fig.gca()\n",
    "\n",
    "\n",
    "show_vector(ax, origin, a, \"$\\\\vec{a}$\")\n",
    "show_vector(ax, origin, b, \"$\\\\vec{b}$\")\n",
    "show_vector(ax, origin, c, \"$\\\\vec{c}$\")\n",
    "\n",
    "show_vector(ax, a, a + b, \"$\\\\vec{a} + \\\\vec{b}$\")\n",
    "show_vector(ax, a + b, a + b + c, \"$\\\\vec{a} + \\\\vec{b} + \\\\vec{c}$\")\n",
    "show_vector(ax, a + b + c, a + b + c - b, \"$\\\\vec{a} + \\\\vec{b} + \\\\vec{c} - \\\\vec{b}$\")\n",
    "\n",
    "\n",
    "ax.set_xlim(-1.5, 2.0)\n",
    "ax.set_ylim(-0.5, 2.0)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend(loc=\"center left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-11",
   "metadata": {},
   "source": [
    "### Tama침o de un vector\n",
    "\n",
    "Una de las formas para calcular la \"magnitud\" o la \"longitud\" de un vector es la siguiente f칩rmula:  \n",
    "\n",
    "$$ \\|\\vec{x}\\|_2 = \\sqrt{x_0^2 + x_1^2 + x_2^2 + \\dots + x_n^2  } $$\n",
    "\n",
    "Esta f칩rmula es conocida como la *norma* de un vector; *NumPy* nos la ofrece a trav칠s de la funci칩n `np.linalg.norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 10.0, -5.0])\n",
    "y = np.array([1.0, -4.0, 8.0])\n",
    "print(np.linalg.norm(x))\n",
    "print(np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-13",
   "metadata": {},
   "source": [
    "Esta norma es conocida como la **norma euclidiana**, y que tambi칠n nos ayuda a calcular la distancia entre dos vectores $\\vec{a}$ y $\\vec{b}$:  \n",
    "\n",
    "$$||\\vec{x}-\\vec{y}||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-14",
   "metadata": {},
   "source": [
    "#### Diferentes tama침os  \n",
    "\n",
    "Como ya lo mencion칠, existen diversas maneras de determinar la magnitud de un vector:\n",
    "\n",
    "| Notaci칩n             | Nombre           | Efecto                        |\n",
    "|----------------------|------------------|-------------------------------|\n",
    "| $\\|x\\|$ or $\\|x\\|_2$ | Norma Euclidiana | Distancia \"ordinaria\"         |\n",
    "| $\\|x\\|_1$            | Norma Manhattan  | Suma de los valores absolutos |\n",
    "| $\\|x\\|_0$            | Pseudo-Norma 0   | N칰mero de valores $\\neq 0$    |\n",
    "| $\\|x\\|_\\inf$         | Norma m치xima     | Elemento mayor                |\n",
    "| $\\|x\\|_{-\\inf}$      | Norma m칤nima     | Elemento menor                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = [2, 0, 5, -5, -1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, 0)  # Pseudo-norma cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, 1)  # Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, np.inf)  # M치ximo absoluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, -np.inf)  # M칤nimo absoluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-22",
   "metadata": {},
   "source": [
    "#### Vectores unitarios  \n",
    "\n",
    "Un vector cuya norma sea $1$ es conocido como vector unitario (esto implica que diferente normas nos llevan a diferentes vectores unitarios).  \n",
    "\n",
    "Cuando nosotros tomamos un vector y lo dividimos entre su norma podemos decir que el vector ha sido normalizado, por ejemplo con la norma euclidiana:  \n",
    "\n",
    "$$\\vec{x}_n = \\vec{x} \\frac{1}{\\|\\vec{x}\\|_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_v(x):\n",
    "    return print(np.around(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(1, 5, 4)\n",
    "norm = np.linalg.norm(x)\n",
    "x_norm = x / norm\n",
    "\n",
    "print_v(x)\n",
    "print(norm)\n",
    "print_v(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-25",
   "metadata": {},
   "source": [
    "#### Representaci칩n gr치fica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1, (50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5), dpi=200)\n",
    "ax = fig.gca()\n",
    "\n",
    "show_normed_vects(ax, x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-28",
   "metadata": {},
   "source": [
    "### Producto interno  \n",
    "\n",
    "Existe otra operacion que nos da la noci칩n del 치ngulo que existe entre dos vectores, esta operaci칩n se conoce como **producto interno**:\n",
    "\n",
    "$$\\cos \\theta = \\frac{\\vec{x} \\bullet \\vec{y}} {||\\vec{x}|| \\  ||\\vec{y}||}$$\n",
    "\n",
    "La operaci칩n en el numerador es la suma de la multiplicaci칩n *elemento-a-elemento*:  \n",
    "\n",
    "$$\\vec{x} \\bullet \\vec{y} = \\sum_i{x_i y_i}$$  \n",
    "\n",
    "Si estamos hablando de vectores unitarios podemos ignorar el denominador (la norma de ambos vectores es $1$.\n",
    "\n",
    "Evidentemente, el producto interno est치 solamente definido para vectores con dimensiones iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [4, 3, 2, 1]\n",
    "\n",
    "print(np.inner(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-30",
   "metadata": {},
   "source": [
    "El producto interno es una generalizaci칩n del producto punto.\n",
    "\n",
    "Este producto nos ayuda cuando queremos comparar un par de vectores, para revisar si es que ambos apuntan a la misma direcci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-31",
   "metadata": {},
   "source": [
    "### Producto externo\n",
    "\n",
    "Existe otra clase de producto que dados dos vectores $\\vec{x}$ y $\\vec{y}$ est치 definido de la siguiente manera:  \n",
    "\n",
    "$$\\vec{x} \\otimes \\vec{y} = \\begin{bmatrix}\n",
    "x_1 y_1 & x_1 y_2 & \\dots & x_1 y_m \\\\\n",
    "x_2 y_1 & x_2 y_2 & \\dots & x_2 y_m \\\\\n",
    "\\dots\\\\\n",
    "x_n y_1 & x_n y_2 & \\dots & x_n y_m \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "S칤, es una matriz cuyas entradas est치n definidas por $A=\\vec{x}\\otimes\\vec{y}$ are: $A_{ij} = x_i  y_j$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-32",
   "metadata": {},
   "source": [
    "### Producto cruz  \n",
    "\n",
    "En tres dimensiones existe un producto llanado **producto cruz**, dados dos vectores $\\vec{a}$ e $\\vec{b}$, el resultado es un tercero que est치 separado exactamente 90 grados de los dos vectores que se usaron para generarlo.  \n",
    "\n",
    "\n",
    "$${\\displaystyle {\\begin{alignedat}{3}\\mathbf {\\vec{a}} &=a_{1}\\mathbf {\\color {blue}{i}} &&+a_{2}\\mathbf {\\color {red}{j}} &&+a_{3}\\mathbf {\\color {lime}{k}}\\end{alignedat}}}$$ \n",
    "\n",
    "$${\\displaystyle {\\begin{alignedat}{3}\\mathbf {\\vec{b}} &=b_{1}\\mathbf {\\color {blue}{i}} &&+b_{2}\\mathbf {\\color {red}{j}} &&+b_{3}\\mathbf {\\color {lime}{k}} \\end{alignedat}}}$$\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\mathbf {\\vec{a} \\times \\vec{b}} &=(a_{2}b_{3}\\mathbf {i} +a_{3}b_{1}\\mathbf {j} +a_{1}b_{2}\\mathbf {k} )-(a_{3}b_{2}\\mathbf {i} +a_{1}b_{3}\\mathbf {j} +a_{2}b_{1}\\mathbf {k} )\\\\&=(a_{2}b_{3}-a_{3}b_{2})\\mathbf {i} +(a_{3}b_{1}-a_{1}b_{3})\\mathbf {j} +(a_{1}b_{2}-a_{2}b_{1})\\mathbf {k}\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-33",
   "metadata": {},
   "source": [
    "## Estad칤sticas sobre vectores  \n",
    "\n",
    "Ya hablamos sobre algunas operaciones en vectores; podemos utilizarlas para definir operaciones estad칤sticas (que existen en el mundo de los n칰meros reales).\n",
    "\n",
    "### Centroide geom칠trico  \n",
    "\n",
    "Supongamos que tenemos una colecci칩n de $N$ vectores, podemos definir el promedio de esta colecci칩n como la suma de todos los vectores dividida entre $N$:  \n",
    "\n",
    "$$mean(\\vec{x_1}, \\vec{x_2}, \\dots, \\vec{x_n}) = \\frac{1}{N} \\sum_i {\\vec{x_i}}$$  \n",
    "En el mundo de los vectores este vector promedio se conoce como el **centroide** del conjunto de $N$ vectores definidos anteriormente.\n",
    "\n",
    "*NumPy* nos hace muy sencillo calcular el vector promedio usando `np.mean(vectors, axis=0)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "centro = 25\n",
    "spread = 5\n",
    "vectors = np.random.normal(centro, spread, (50, 2))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10), dpi=200)\n",
    "ax = fig.gca()\n",
    "\n",
    "centroid = np.mean(vectors, axis=0)\n",
    "\n",
    "ax.scatter(vectors[:, 0], vectors[:, 1], label=\"Vectores\")\n",
    "ax.scatter([centroid[0]], [centroid[1]], s=50, label=\"Centroid\")\n",
    "ax.legend()\n",
    "# ax.set_xlim((centro-spread, centro+spread))\n",
    "# ax.set_ylim((centro-spread, centro+spread))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-35",
   "metadata": {},
   "source": [
    "### Centrando un dataset    \n",
    "\n",
    "Tomando el centroide, podemos realizar una operaci칩n muy importante conocida como **centrar un dataset** si tomamos todos los vectores de nuestro conjunto y le restamos este centroide. Esto nos va a llevar a que nuestro nuevo conunto de vectores (llam칠mosle \"centrado\") tiene un **promedio cero**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=200)\n",
    "ax = fig.gca()\n",
    "ax.axhline(0, c=\"k\", ls=\":\")\n",
    "ax.axvline(0, c=\"k\", ls=\":\")\n",
    "\n",
    "centroid = np.mean(vectors, axis=0)\n",
    "centrado = vectors - centroid\n",
    "new_centroid = np.mean(centrado, axis=0)\n",
    "\n",
    "\n",
    "ax.scatter(vectors[:, 0], vectors[:, 1], label=\"Vectores originales\")\n",
    "ax.scatter([centroid[0]], [centroid[1]], s=50, label=\"Centroide original\")\n",
    "\n",
    "ax.arrow(\n",
    "    centroid[0],\n",
    "    centroid[1],\n",
    "    new_centroid[0] - centroid[0],\n",
    "    new_centroid[1] - centroid[1],\n",
    "    head_width=1,\n",
    "    linestyle=\"-\",\n",
    "    width=0.2,\n",
    "    overhang=1,\n",
    "    length_includes_head=True,\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "ax.scatter(centrado[:, 0], centrado[:, 1], label=\"Vectores centrados\")\n",
    "ax.scatter([new_centroid[0]], [new_centroid[1]], s=50, label=\"Nuevo centroide\")\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-37",
   "metadata": {},
   "source": [
    "### Midiendo la extensi칩n (*Measuring the spread*)  \n",
    "\n",
    "Generalizando el concepto de varianza (que nos ayuda a medir la extensi칩n de un conjunto de datos) podemos encontar la extensi칩n de nuestro conjunto de datos en el caso multidimensional. Recordando la forma unidimensional:  \n",
    "\n",
    "$$\\sigma^2 =  \\frac{1}{N-1} \\sum_{i=0}^{N-1}{(x_i - \\mu)^2}$$\n",
    "\n",
    "(La varianza esl promedio de las sumas del cuadrado de las diferencias de cada elemento con el promedio...)\n",
    "\n",
    "De la varianza podemos obtener la desviaci칩n estandar si calculamos la raiz cuadrada de $\\sigma$.\n",
    "\n",
    "En el caso multidimensional, tenemos que calcular algo conocido como **covarianza**, esta covarianza nos dice qu칠 tan ... \n",
    "\n",
    "Este es el promedio de las diferencias de cada elemento de una columna con el promedio del resto de las columnas:                                                   \n",
    "\n",
    "$$\\Sigma_{ij} = \\frac{1}{N-1}  \\sum_{k=1}^{N} (X_{ki}-\\mu_i)(X_{kj}-\\mu_j)$$\n",
    "\n",
    "En *NumPy* podemos calcular la covarianza con `np.cov`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(2, 1, (100, 2)) @ np.array(\n",
    "    [[0.5, 0.1], [-0.9, -3.0]]\n",
    ")  # 30 filas y 4 columnas\n",
    "\n",
    "sigma_np = np.cov(x, rowvar=False)\n",
    "sigma_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-39",
   "metadata": {},
   "source": [
    "#### Elipses de covarianza  \n",
    "\n",
    "Podemos ver gr치ficamente la covarianza a trav칠s de elipses; este elipse parece \"capturar\" todo nuestro dataset. El centroide del dataset es el centro del elipse y la matriz de covarianza representa la forma del mismo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), dpi=200)\n",
    "ax = fig.gca()\n",
    "ax.scatter(x[:, 0], x[:, 1], s=5)\n",
    "draw_covariance_ellipse(ax, x, 0.5)\n",
    "draw_covariance_ellipse(ax, x, 1.0)\n",
    "draw_covariance_ellipse(ax, x, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-41",
   "metadata": {},
   "source": [
    "## Espacios vectoriales de grandes dimensiones\n",
    "\n",
    "Trabajar con informaci칩n representada en forma de arreglos de una, dos y hasta tres dimensiones es sencillo: estamos familiarizados con ella, podemos graficarla, es f치cil visualizarla mentalmente... sin embargo, muchas veces, la ciencia de datos involucra espacios vectoriales de grandes dimensiones.   \n",
    "\n",
    "Piensa en una imagen peque침a, de unos 500 pixeles de ancho por 500 de alto, esta informaci칩n es representada en un vector de $500 \\times 500 \\times 3 = 750,000$ entradas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-42",
   "metadata": {},
   "source": [
    "### La maldici칩n de la dimensionalidad 游땸  \n",
    "\n",
    "쮺칩mo se escucha trabajar con vectores de 1000, 5000, 10000 elementos? \n",
    "\n",
    "\n",
    "Hay algoritmos que se desempe침an perfectamente cuando operamos con vectores en espacios de pocas dimensiones pero cuando aumentamos la cantidad de elementos en los vectores comienzan a fallar.\n",
    "\n",
    "La **maldici칩n de la dimensionalidad** hace que buscar en un espacio sea complejo afectando a los algoritmos que funcionan particionando el dataset: 츼rboles de decisi칩n, Redes Neuronales, M치quinas de Vectores, Nearest Neighbours, K-means...\n",
    "\n",
    "Entre m치s dimensiones, es m치s dif칤cil generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-43",
   "metadata": {},
   "source": [
    "## Matrices  \n",
    "\n",
    "Ya habiamos hablado sobre matrices, ahora vamos a ver algunas de las operaciones que podemos hacer con algo conocido como 치lgebra de matrices, tambi칠n conocida como 치lgebra lineal:  \n",
    "\n",
    "### Suma de matrices\n",
    "\n",
    "$$\\begin{equation} A + B = \\begin{bmatrix}\n",
    "a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \\dots & a_{1,m} + b_{1,m} \\\\\n",
    "a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \\dots & a_{2,m} + b_{2,m} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots  \\\\\n",
    "a_{n,1} + b_{n,1} & a_{n,2} + b_{n,2} & \\dots & a_{n,m} + b_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$  \n",
    "\n",
    "### Multiplicaci칩n por escalar  \n",
    "\n",
    "$$\\begin{equation} cA = \\begin{bmatrix}\n",
    "ca_{1,1}  & ca_{1,2} & \\dots & ca_{1,m}  \\\\\n",
    "ca_{2,1} & ca_{2,2}  & \\dots & ca_{2,m} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots  \\\\\n",
    "ca_{n,1}  & ca_{n,2} & \\dots & ca_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "### Multiplicaci칩n entre matrices   \n",
    "\n",
    "El resultado de una multiplicacion de matrices $A$ y $B$ es otra matriz: \n",
    "\n",
    "$$AB = C$$  \n",
    "\n",
    "La multiplicaci칩n de matrices solamente est치 definida para matrices $A$ y $B$ si:  \n",
    "\n",
    " - $A$ tiene dimensiones $p \\times q$\n",
    " - $B$ tiene dimensiones $q \\times r$  \n",
    "\n",
    "Puedes ver la multiplicaci칩n de matrices definida con la siguiente funci칩n:  \n",
    "\n",
    "$$\\begin{equation}C_{ij}=\\sum_k a_{ik} b_{kj} \\end{equation}$$  \n",
    "\n",
    "La multiplicaci칩n de matrices viene integrada en *NumPy*:  \n",
    "\n",
    " - `np.dot(A, B)`  \n",
    " - `A @ B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(0, 10, (3, 5))\n",
    "B = np.random.randint(0, 10, (5, 2))\n",
    "\n",
    "dot_result = np.dot(A, B)\n",
    "at_result = A @ B\n",
    "\n",
    "print(A.shape, B.shape)\n",
    "print()\n",
    "print(dot_result)\n",
    "print()\n",
    "print(at_result)\n",
    "print()\n",
    "print(np.allclose(dot_result, at_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-45",
   "metadata": {},
   "source": [
    "### Matrices y vectores  \n",
    "\n",
    "Tambi칠n podemos multiplicar matrices por un vector... de nuevo, las reglas anteriormente definidas aplican:  \n",
    "\n",
    "$$\\vec{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\dots\\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "El vector de ac치 arriba se conoce como **vector columna** de dimensi칩n $D \\times 1$, mientras que el de abajo se conoce como **vector fila** de dimensi칩n $1 \\times D$: \n",
    "\n",
    "$$\\vec{y} =\n",
    "\\begin{bmatrix}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\dots & \n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dada nuestra definici칩n de multiplicaci칩n, multiplicar $\\vec{x}\\vec{y}$ resultar치 en una matriz de $D \\times D$ (el producto externo 游땔), mientras que multiplicar $\\vec{y}\\vec{x}$ resultar치 en un escalar (el producto interno 游땔):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3]])\n",
    "y = np.array([[4, 5, 6]])\n",
    "\n",
    "print(\"x\", x.shape)\n",
    "print(x, \"\\n\")\n",
    "print(\"y\", y.shape)\n",
    "print(y, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.outer(x, y))\n",
    "print(x.T @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.inner(y, x))\n",
    "print(y @ x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicaci칩n matricial\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "A = np.array([[1, 0, 1], [1, 0, 0], [0, 1, 1]])  # 3x3\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print()\n",
    "print(A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "\n",
    "\n",
    "print(np.dot(x, x))\n",
    "print(x @ x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-51",
   "metadata": {},
   "source": [
    " > 丘멆잺 A diferencia de con los n칰meros reales, la mutliplicaci칩n de matrices no es conmutativa:\n",
    "        \n",
    "$$AB \\neq BA$$\n",
    "\n",
    " > 丘멆잺 A칰n que las dimensiones sean compatibles, en general al multiplicaci칩n de matrices no es conmutativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-52",
   "metadata": {},
   "source": [
    "### Exponenciaci칩n de matrices  \n",
    "\n",
    "Una vez que tenemos a nuestra disposici칩n la multiplicaci칩n, podemos definir el hecho de multiplicar una matriz **cuadrada** por si misma como la exponenciaci칩n matricial:  \n",
    "\n",
    "$$A^4=AAAA$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-53",
   "metadata": {},
   "source": [
    "## Matrices especiales  \n",
    "\n",
    "### Matriz diagonal  \n",
    "\n",
    "Matrices para las que los elementos $A_{ij} == 0$ en donde $i \\neq j$ se llaman matrices diagonales, tienen significado especial en el 치lgebra lineal.  \n",
    "\n",
    "Podemos convertir un vector de longitud $D$ en una matriz de $D \\times D$ con los elementos del vector en la diagonal usando `np.diag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (4,))\n",
    "diag_x = np.diag(x)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-55",
   "metadata": {},
   "source": [
    "Es posible extraer el vector de la diagonal de una matriz usando... `np.diag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (4, 4))\n",
    "diag_x = np.diag(x)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-57",
   "metadata": {},
   "source": [
    "### Matriz identidad  \n",
    "\n",
    "Una matriz cuadrada cuyos elementos son $0$ en todas partes excepto en la diagonal en donde los valores son $1$ es conocida como una matriz identidad.  \n",
    "\n",
    "La matriz identidad no provoca ning칰n cambio cuando la usamos en una multiplicaci칩n. En *NumPy* podemos usar `np.eye(n)` para generar una matriz identidad de tama침o `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (3, 3))\n",
    "I = np.eye(3, dtype=int)\n",
    "\n",
    "print(x)\n",
    "print(x @ I)\n",
    "print(I @ x)\n",
    "# print(I * x) # Not the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-59",
   "metadata": {},
   "source": [
    "### Matriz cero  \n",
    "\n",
    "Ah... lo que dice el t칤tulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-60",
   "metadata": {},
   "source": [
    "### Matriz cuadrada  \n",
    "\n",
    "游쓇릢餃쮪잺郊勇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-61",
   "metadata": {},
   "source": [
    "### Matriz triangular  \n",
    "\n",
    "Una matriz es considerada triangular si los elementos debajo o arriba de la diagonal son cero. \n",
    "\n",
    "Existen dos subtipos:  \n",
    "\n",
    " - **Triangular superior**\n",
    " \n",
    "$$\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "0 & 5 & 6 & 7 \\\\\n",
    "0 & 0 & 8 & 9 \\\\\n",
    "0 & 0 & 0 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    " \n",
    " - **Triangular inferior**  \n",
    " \n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "2 & 3 & 0 & 0 \\\\\n",
    "4 & 5 & 6 & 0 \\\\\n",
    "7 & 8 & 9 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Podemos usar `np.tri(n)` para generar una matriz triangular inferior de $1$ usando *NumPy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangular = np.tri(4)\n",
    "\n",
    "print(triangular)\n",
    "print()\n",
    "print(triangular.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-63",
   "metadata": {},
   "source": [
    "### Matrices dispersas  \n",
    "\n",
    "A pesar de que no hemos hablado de estas matrices (ni hablaremos en el futuro en este curso) sobre matrices dispersas, son una parte central en la ciencia de datos.  \n",
    "\n",
    "Imagina que en Spotify tienen una matriz en donde las filas son las todas las canciones y las columnas los usuarios, y los valores dentro de la matriz representan si al usuario le gusta o no dicha canci칩n, 쯖칩mo crees que se ver칤a esa matriz?\n",
    "\n",
    "|                    | Feregrino | Alma      | Benito    | ... | Terry     | Destiny   |\n",
    "|--------------------|-----------|-----------|-----------|-----|-----------|-----------|\n",
    "| La Puerta Negra    | ${\\bf 1}$ | $0$       | $0$       |     | $0$       | $0$       |\n",
    "| Flux               | $0$       | ${\\bf 1}$ | $0$       |     | ${\\bf 1}$ | $0$       |\n",
    "| La mesa del rinc칩n | ${\\bf 1}$ | $0$       | $0$       |     | $0$       | $0$       |\n",
    "| Andar conmigo      | ${\\bf 1}$ | ${\\bf 1}$ | $0$       |     | ${\\bf 1}$ | $0$       |\n",
    "| ...                |           |           |           | ... |           |           |\n",
    "| Dark Horse         | $0$       | ${\\bf 1}$ | ${\\bf 1}$ |     | $0$       | ${\\bf 1}$ |\n",
    "\n",
    "Lo m치s seguro es que la gran mayor칤a de las personas no disfruten de la gran mayor칤a de las canciones, entonces la matriz de ac치 arriba muy probablemente estar치 dominada por $0$, con muy pocos $1$ esparcidos aqu칤 y all치.\n",
    "\n",
    "쮺칩mo lidiar칤as con ella?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-64",
   "metadata": {},
   "source": [
    "---------\n",
    "## Resources\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtu.be/bgEML8Re2ks\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "### Sitios web\n",
    "\n",
    " - **Espacios y subespacios vectoriales** - [https://aga.frba.utn.edu.ar/espacios-y-subespacios-vectoriales/](https://aga.frba.utn.edu.ar/espacios-y-subespacios-vectoriales/) \n",
    " \n",
    " - **High-Dimensional Space** - [https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf)\n",
    " \n",
    " - **Maldici칩n de la dimensi칩n** - [https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)  \n",
    " \n",
    " - **List of named matrices** - [https://en.wikipedia.org/wiki/List_of_named_matrices](https://en.wikipedia.org/wiki/List_of_named_matrices)\n",
    "\n",
    "### Videos\n",
    "\n",
    " - **3blue1brown Linear Algebra series** -[https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)  \n",
    "\n",
    "### Libros\n",
    "\n",
    " - **Introduction to applied linear algebra** - [http://stanford.edu/~boyd/vmls/vmls.pdf](http://stanford.edu/~boyd/vmls/vmls.pdf)\n",
    "\n",
    " - **Coding the matrix** - [https://codingthematrix.com/](https://codingthematrix.com/)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
