{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-0",
   "metadata": {},
   "source": [
    "# 05: Álgebra Lineal Computacional (Vectores)\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtu.be/bgEML8Re2ks\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-1",
   "metadata": {},
   "source": [
    "## Paquetes  \n",
    "\n",
    " - `numpy`\n",
    " - `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from df.display_algebra import show_vector, show_normed_vects, draw_covariance_ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-3",
   "metadata": {},
   "source": [
    "## Espacios vectoriales  \n",
    "\n",
    "Supongamos que tenemos un conjunto de vectores $[x_1, x_2, \\dots x_d], x_i \\in \\mathbb{R}$ (vamos a hablar sobre números reales, pero podemos generalizar a cualquier otro conjunto de números) de una dimensión $d$, nosotros podríamos decir que este conjunto de vectores existe dentro de un espacio vectorial si se cumplen ciertas condiciones...   \n",
    "\n",
    "Digamos que tenemos los vectores:   \n",
    "\n",
    "$$\\vec{x} = [x_1, x_2, \\dots, x_d]$$  \n",
    "$$\\vec{y} = [y_1, y_2, \\dots, y_d]$$\n",
    "\n",
    "### Operaciones en espacios vectoriales  \n",
    "\n",
    "Entre las condiciones que deben cumplirse están que debemos tener bien definidas dos operaciones:  \n",
    "\n",
    " - **multiplicación por un escalar**: de tal modo que $\\alpha \\vec{x}$ está definido para cualquier número escalar $\\alpha$. En nuestro caso de los reales, $\\alpha \\vec{x} = [\\alpha x_1, \\alpha x_2, \\dots, \\alpha x_d]$  \n",
    " \n",
    " - **suma de vectores**: de tal modo que $\\vec{x} + \\vec{y}$, En nuestro caso de los reales, $\\vec{x} + \\vec{y} = [x_1 + y_1, x_2 + y_2, \\dots x_d + y_d]$\n",
    " \n",
    "<small>Existen 10 axiomas basadas en estas dos operaciones que deben cumplirse.</small>\n",
    "\n",
    "\n",
    "Existe un subconjunto de espacios vectoriales, llamado **espacio vectorial topológico** bajo los cuales debemos definir otro par de operaciones:    \n",
    "\n",
    " - **norma**: $||\\vec{x}||$ que define la longitud de un vector, útil para medir y comparar vectores \n",
    "\n",
    " - **producto interno**: $\\langle \\vec{x} | \\vec{y} \\rangle$ or $\\vec{x} \\bullet \\vec{y}$  which allows the angles of two vectors to be compared. The inner product of two orthogonal vectors is 0. For real vectors $\\vec{x} \\bullet \\vec{y} = x_1 y_1 + x_2 y_2 + x_3 y_3 \\dots x_d y_d$\n",
    "  \n",
    "Las operaciones que definimos son importantes para la ciencia de datos porque nos permite hablar de que un par de vectores están cerca el uno del otro, nos permite sumarlos, compararlos y escalarlos.\n",
    "\n",
    "<img src=\"assets/05/vectores.png\" />\n",
    "\n",
    "### Recordando *NumPy*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([0, 1, 1])\n",
    "y = np.array([4, 5, 6])\n",
    "a = 2\n",
    "\n",
    "\n",
    "def pp(text, value):\n",
    "    print(text.rjust(10), \"=\", str(value))\n",
    "\n",
    "\n",
    "pp(\"a\", a)\n",
    "pp(\"x\", x)\n",
    "pp(\"y\", y)\n",
    "pp(\"a * x\", a * x)\n",
    "pp(\"x + y\", x + y)\n",
    "pp(\"||x||\", np.linalg.norm(x))\n",
    "pp(\"x · y\", np.dot(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-5",
   "metadata": {},
   "source": [
    "- **`np.linalg.norm`**  \n",
    "- **`np.dot`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-6",
   "metadata": {},
   "source": [
    "### Información como vectores  \n",
    "\n",
    "Las representaciones vectoriales son de vital importancia en el aprendizaje automático, puesto que la información es más fácilmente procesada por un algoritmo cuando es representada en forma de vectores.\n",
    "\n",
    "Pero antes de hablar de vectores, es importante entender que muchas veces la información no está en forma vectorial por naturaleza, en muchos casos el proceso es el siguiente:  \n",
    "\n",
    "<img src=\"assets/05/experiment.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-7",
   "metadata": {},
   "source": [
    "### Un clásico de machine learning...   \n",
    "\n",
    "<img src=\"assets/05/flower.png\" />\n",
    "\n",
    "<small>Imagen original en el curso de Data Fundamentals <a href=\"https://www.gla.ac.uk/undergraduate/degrees/computingscience/?card=course&code=COMPSCI4073\">https://www.gla.ac.uk/undergraduate/degrees/computingscience/?card=course&code=COMPSCI4073</a></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-8",
   "metadata": {},
   "source": [
    "## Operaciones vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-9",
   "metadata": {},
   "source": [
    "### Sumas y multiplicaciones  \n",
    "\n",
    "Ya hablamos de las sumas y multiplicaciones *elemento-elemento*; una consecuencia de esto es que podemos introducir el concepto de sumas ponderadas:   \n",
    "\n",
    "$$\\lambda_1 x_1 + \\lambda_2 x_2 + \\dots + \\lambda_n x_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1.0, 1.0])\n",
    "b = np.array([0.0, 0.5])\n",
    "c = np.array([0.5, 0.0])\n",
    "\n",
    "origin = np.array([0, 0])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5), dpi=200)\n",
    "ax = fig.gca()\n",
    "\n",
    "\n",
    "show_vector(ax, origin, a, \"$\\\\vec{a}$\")\n",
    "show_vector(ax, origin, b, \"$\\\\vec{b}$\")\n",
    "show_vector(ax, origin, c, \"$\\\\vec{c}$\")\n",
    "\n",
    "show_vector(ax, a, a + b, \"$\\\\vec{a} + \\\\vec{b}$\")\n",
    "show_vector(ax, a + b, a + b + c, \"$\\\\vec{a} + \\\\vec{b} + \\\\vec{c}$\")\n",
    "show_vector(ax, a + b + c, a + b + c - b, \"$\\\\vec{a} + \\\\vec{b} + \\\\vec{c} - \\\\vec{b}$\")\n",
    "\n",
    "\n",
    "ax.set_xlim(-1.5, 2.0)\n",
    "ax.set_ylim(-0.5, 2.0)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend(loc=\"center left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-11",
   "metadata": {},
   "source": [
    "### Tamaño de un vector\n",
    "\n",
    "Una de las formas para calcular la \"magnitud\" o la \"longitud\" de un vector es la siguiente fórmula:  \n",
    "\n",
    "$$ \\|\\vec{x}\\|_2 = \\sqrt{x_0^2 + x_1^2 + x_2^2 + \\dots + x_n^2  } $$\n",
    "\n",
    "Esta fórmula es conocida como la *norma* de un vector; *NumPy* nos la ofrece a través de la función `np.linalg.norm`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 10.0, -5.0])\n",
    "y = np.array([1.0, -4.0, 8.0])\n",
    "print(np.linalg.norm(x))\n",
    "print(np.linalg.norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-13",
   "metadata": {},
   "source": [
    "Esta norma es conocida como la **norma euclidiana**, y que también nos ayuda a calcular la distancia entre dos vectores $\\vec{a}$ y $\\vec{b}$:  \n",
    "\n",
    "$$||\\vec{x}-\\vec{y}||_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-14",
   "metadata": {},
   "source": [
    "#### Diferentes tamaños  \n",
    "\n",
    "Como ya lo mencioné, existen diversas maneras de determinar la magnitud de un vector:\n",
    "\n",
    "| Notación             | Nombre           | Efecto                        |\n",
    "|----------------------|------------------|-------------------------------|\n",
    "| $\\|x\\|$ or $\\|x\\|_2$ | Norma Euclidiana | Distancia \"ordinaria\"         |\n",
    "| $\\|x\\|_1$            | Norma Manhattan  | Suma de los valores absolutos |\n",
    "| $\\|x\\|_0$            | Pseudo-Norma 0   | Número de valores $\\neq 0$    |\n",
    "| $\\|x\\|_\\inf$         | Norma máxima     | Elemento mayor                |\n",
    "| $\\|x\\|_{-\\inf}$      | Norma mínima     | Elemento menor                |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vector = [2, 0, 5, -5, -1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, 0)  # Pseudo-norma cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, 1)  # Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, np.inf)  # Máximo absoluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, -np.inf)  # Mínimo absoluto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(test_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-22",
   "metadata": {},
   "source": [
    "#### Vectores unitarios  \n",
    "\n",
    "Un vector cuya norma sea $1$ es conocido como vector unitario (esto implica que diferente normas nos llevan a diferentes vectores unitarios).  \n",
    "\n",
    "Cuando nosotros tomamos un vector y lo dividimos entre su norma podemos decir que el vector ha sido normalizado, por ejemplo con la norma euclidiana:  \n",
    "\n",
    "$$\\vec{x}_n = \\vec{x} \\frac{1}{\\|\\vec{x}\\|_2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_v(x):\n",
    "    return print(np.around(x, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(1, 5, 4)\n",
    "norm = np.linalg.norm(x)\n",
    "x_norm = x / norm\n",
    "\n",
    "print_v(x)\n",
    "print(norm)\n",
    "print_v(x_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-25",
   "metadata": {},
   "source": [
    "#### Representación gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1, (50, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5), dpi=200)\n",
    "ax = fig.gca()\n",
    "\n",
    "show_normed_vects(ax, x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-28",
   "metadata": {},
   "source": [
    "### Producto interno  \n",
    "\n",
    "Existe otra operacion que nos da la noción del ángulo que existe entre dos vectores, esta operación se conoce como **producto interno**:\n",
    "\n",
    "$$\\cos \\theta = \\frac{\\vec{x} \\bullet \\vec{y}} {||\\vec{x}|| \\  ||\\vec{y}||}$$\n",
    "\n",
    "La operación en el numerador es la suma de la multiplicación *elemento-a-elemento*:  \n",
    "\n",
    "$$\\vec{x} \\bullet \\vec{y} = \\sum_i{x_i y_i}$$  \n",
    "\n",
    "Si estamos hablando de vectores unitarios podemos ignorar el denominador (la norma de ambos vectores es $1$.\n",
    "\n",
    "Evidentemente, el producto interno está solamente definido para vectores con dimensiones iguales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [4, 3, 2, 1]\n",
    "\n",
    "print(np.inner(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-30",
   "metadata": {},
   "source": [
    "El producto interno es una generalización del producto punto.\n",
    "\n",
    "Este producto nos ayuda cuando queremos comparar un par de vectores, para revisar si es que ambos apuntan a la misma dirección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-31",
   "metadata": {},
   "source": [
    "### Producto externo\n",
    "\n",
    "Existe otra clase de producto que dados dos vectores $\\vec{x}$ y $\\vec{y}$ está definido de la siguiente manera:  \n",
    "\n",
    "$$\\vec{x} \\otimes \\vec{y} = \\begin{bmatrix}\n",
    "x_1 y_1 & x_1 y_2 & \\dots & x_1 y_m \\\\\n",
    "x_2 y_1 & x_2 y_2 & \\dots & x_2 y_m \\\\\n",
    "\\dots\\\\\n",
    "x_n y_1 & x_n y_2 & \\dots & x_n y_m \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Sí, es una matriz cuyas entradas están definidas por $A=\\vec{x}\\otimes\\vec{y}$ are: $A_{ij} = x_i  y_j$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-32",
   "metadata": {},
   "source": [
    "### Producto cruz  \n",
    "\n",
    "En tres dimensiones existe un producto llanado **producto cruz**, dados dos vectores $\\vec{a}$ e $\\vec{b}$, el resultado es un tercero que está separado exactamente 90 grados de los dos vectores que se usaron para generarlo.  \n",
    "\n",
    "\n",
    "$${\\displaystyle {\\begin{alignedat}{3}\\mathbf {\\vec{a}} &=a_{1}\\mathbf {\\color {blue}{i}} &&+a_{2}\\mathbf {\\color {red}{j}} &&+a_{3}\\mathbf {\\color {lime}{k}}\\end{alignedat}}}$$ \n",
    "\n",
    "$${\\displaystyle {\\begin{alignedat}{3}\\mathbf {\\vec{b}} &=b_{1}\\mathbf {\\color {blue}{i}} &&+b_{2}\\mathbf {\\color {red}{j}} &&+b_{3}\\mathbf {\\color {lime}{k}} \\end{alignedat}}}$$\n",
    "\n",
    "$${\\displaystyle {\\begin{aligned}\\mathbf {\\vec{a} \\times \\vec{b}} &=(a_{2}b_{3}\\mathbf {i} +a_{3}b_{1}\\mathbf {j} +a_{1}b_{2}\\mathbf {k} )-(a_{3}b_{2}\\mathbf {i} +a_{1}b_{3}\\mathbf {j} +a_{2}b_{1}\\mathbf {k} )\\\\&=(a_{2}b_{3}-a_{3}b_{2})\\mathbf {i} +(a_{3}b_{1}-a_{1}b_{3})\\mathbf {j} +(a_{1}b_{2}-a_{2}b_{1})\\mathbf {k}\\end{aligned}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-33",
   "metadata": {},
   "source": [
    "## Estadísticas sobre vectores  \n",
    "\n",
    "Ya hablamos sobre algunas operaciones en vectores; podemos utilizarlas para definir operaciones estadísticas (que existen en el mundo de los números reales).\n",
    "\n",
    "### Centroide geométrico  \n",
    "\n",
    "Supongamos que tenemos una colección de $N$ vectores, podemos definir el promedio de esta colección como la suma de todos los vectores dividida entre $N$:  \n",
    "\n",
    "$$mean(\\vec{x_1}, \\vec{x_2}, \\dots, \\vec{x_n}) = \\frac{1}{N} \\sum_i {\\vec{x_i}}$$  \n",
    "En el mundo de los vectores este vector promedio se conoce como el **centroide** del conjunto de $N$ vectores definidos anteriormente.\n",
    "\n",
    "*NumPy* nos hace muy sencillo calcular el vector promedio usando `np.mean(vectors, axis=0)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "centro = 25\n",
    "spread = 5\n",
    "vectors = np.random.normal(centro, spread, (50, 2))\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10), dpi=200)\n",
    "ax = fig.gca()\n",
    "\n",
    "centroid = np.mean(vectors, axis=0)\n",
    "\n",
    "ax.scatter(vectors[:, 0], vectors[:, 1], label=\"Vectores\")\n",
    "ax.scatter([centroid[0]], [centroid[1]], s=50, label=\"Centroid\")\n",
    "ax.legend()\n",
    "# ax.set_xlim((centro-spread, centro+spread))\n",
    "# ax.set_ylim((centro-spread, centro+spread))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-35",
   "metadata": {},
   "source": [
    "### Centrando un dataset    \n",
    "\n",
    "Tomando el centroide, podemos realizar una operación muy importante conocida como **centrar un dataset** si tomamos todos los vectores de nuestro conjunto y le restamos este centroide. Esto nos va a llevar a que nuestro nuevo conunto de vectores (llamémosle \"centrado\") tiene un **promedio cero**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), dpi=200)\n",
    "ax = fig.gca()\n",
    "ax.axhline(0, c=\"k\", ls=\":\")\n",
    "ax.axvline(0, c=\"k\", ls=\":\")\n",
    "\n",
    "centroid = np.mean(vectors, axis=0)\n",
    "centrado = vectors - centroid\n",
    "new_centroid = np.mean(centrado, axis=0)\n",
    "\n",
    "\n",
    "ax.scatter(vectors[:, 0], vectors[:, 1], label=\"Vectores originales\")\n",
    "ax.scatter([centroid[0]], [centroid[1]], s=50, label=\"Centroide original\")\n",
    "\n",
    "ax.arrow(\n",
    "    centroid[0],\n",
    "    centroid[1],\n",
    "    new_centroid[0] - centroid[0],\n",
    "    new_centroid[1] - centroid[1],\n",
    "    head_width=1,\n",
    "    linestyle=\"-\",\n",
    "    width=0.2,\n",
    "    overhang=1,\n",
    "    length_includes_head=True,\n",
    "    color=\"black\",\n",
    ")\n",
    "\n",
    "ax.scatter(centrado[:, 0], centrado[:, 1], label=\"Vectores centrados\")\n",
    "ax.scatter([new_centroid[0]], [new_centroid[1]], s=50, label=\"Nuevo centroide\")\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-37",
   "metadata": {},
   "source": [
    "### Midiendo la extensión (*Measuring the spread*)  \n",
    "\n",
    "Generalizando el concepto de varianza (que nos ayuda a medir la extensión de un conjunto de datos) podemos encontar la extensión de nuestro conjunto de datos en el caso multidimensional. Recordando la forma unidimensional:  \n",
    "\n",
    "$$\\sigma^2 =  \\frac{1}{N-1} \\sum_{i=0}^{N-1}{(x_i - \\mu)^2}$$\n",
    "\n",
    "(La varianza esl promedio de las sumas del cuadrado de las diferencias de cada elemento con el promedio...)\n",
    "\n",
    "De la varianza podemos obtener la desviación estandar si calculamos la raiz cuadrada de $\\sigma$.\n",
    "\n",
    "En el caso multidimensional, tenemos que calcular algo conocido como **covarianza**, esta covarianza nos dice qué tan ... \n",
    "\n",
    "Este es el promedio de las diferencias de cada elemento de una columna con el promedio del resto de las columnas:                                                   \n",
    "\n",
    "$$\\Sigma_{ij} = \\frac{1}{N-1}  \\sum_{k=1}^{N} (X_{ki}-\\mu_i)(X_{kj}-\\mu_j)$$\n",
    "\n",
    "En *NumPy* podemos calcular la covarianza con `np.cov`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(2, 1, (100, 2)) @ np.array(\n",
    "    [[0.5, 0.1], [-0.9, -3.0]]\n",
    ")  # 30 filas y 4 columnas\n",
    "\n",
    "sigma_np = np.cov(x, rowvar=False)\n",
    "sigma_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-39",
   "metadata": {},
   "source": [
    "#### Elipses de covarianza  \n",
    "\n",
    "Podemos ver gráficamente la covarianza a través de elipses; este elipse parece \"capturar\" todo nuestro dataset. El centroide del dataset es el centro del elipse y la matriz de covarianza representa la forma del mismo:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), dpi=200)\n",
    "ax = fig.gca()\n",
    "ax.scatter(x[:, 0], x[:, 1], s=5)\n",
    "draw_covariance_ellipse(ax, x, 0.5)\n",
    "draw_covariance_ellipse(ax, x, 1.0)\n",
    "draw_covariance_ellipse(ax, x, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-41",
   "metadata": {},
   "source": [
    "## Espacios vectoriales de grandes dimensiones\n",
    "\n",
    "Trabajar con información representada en forma de arreglos de una, dos y hasta tres dimensiones es sencillo: estamos familiarizados con ella, podemos graficarla, es fácil visualizarla mentalmente... sin embargo, muchas veces, la ciencia de datos involucra espacios vectoriales de grandes dimensiones.   \n",
    "\n",
    "Piensa en una imagen pequeña, de unos 500 pixeles de ancho por 500 de alto, esta información es representada en un vector de $500 \\times 500 \\times 3 = 750,000$ entradas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-42",
   "metadata": {},
   "source": [
    "### La maldición de la dimensionalidad 😱  \n",
    "\n",
    "¿Cómo se escucha trabajar con vectores de 1000, 5000, 10000 elementos? \n",
    "\n",
    "\n",
    "Hay algoritmos que se desempeñan perfectamente cuando operamos con vectores en espacios de pocas dimensiones pero cuando aumentamos la cantidad de elementos en los vectores comienzan a fallar.\n",
    "\n",
    "La **maldición de la dimensionalidad** hace que buscar en un espacio sea complejo afectando a los algoritmos que funcionan particionando el dataset: Árboles de decisión, Redes Neuronales, Máquinas de Vectores, Nearest Neighbours, K-means...\n",
    "\n",
    "Entre más dimensiones, es más difícil generalizar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-43",
   "metadata": {},
   "source": [
    "## Matrices  \n",
    "\n",
    "Ya habiamos hablado sobre matrices, ahora vamos a ver algunas de las operaciones que podemos hacer con algo conocido como álgebra de matrices, también conocida como álgebra lineal:  \n",
    "\n",
    "### Suma de matrices\n",
    "\n",
    "$$\\begin{equation} A + B = \\begin{bmatrix}\n",
    "a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \\dots & a_{1,m} + b_{1,m} \\\\\n",
    "a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \\dots & a_{2,m} + b_{2,m} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots  \\\\\n",
    "a_{n,1} + b_{n,1} & a_{n,2} + b_{n,2} & \\dots & a_{n,m} + b_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$  \n",
    "\n",
    "### Multiplicación por escalar  \n",
    "\n",
    "$$\\begin{equation} cA = \\begin{bmatrix}\n",
    "ca_{1,1}  & ca_{1,2} & \\dots & ca_{1,m}  \\\\\n",
    "ca_{2,1} & ca_{2,2}  & \\dots & ca_{2,m} \\\\\n",
    "\\dots & \\dots & \\dots & \\dots  \\\\\n",
    "ca_{n,1}  & ca_{n,2} & \\dots & ca_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "### Multiplicación entre matrices   \n",
    "\n",
    "El resultado de una multiplicacion de matrices $A$ y $B$ es otra matriz: \n",
    "\n",
    "$$AB = C$$  \n",
    "\n",
    "La multiplicación de matrices solamente está definida para matrices $A$ y $B$ si:  \n",
    "\n",
    " - $A$ tiene dimensiones $p \\times q$\n",
    " - $B$ tiene dimensiones $q \\times r$  \n",
    "\n",
    "Puedes ver la multiplicación de matrices definida con la siguiente función:  \n",
    "\n",
    "$$\\begin{equation}C_{ij}=\\sum_k a_{ik} b_{kj} \\end{equation}$$  \n",
    "\n",
    "La multiplicación de matrices viene integrada en *NumPy*:  \n",
    "\n",
    " - `np.dot(A, B)`  \n",
    " - `A @ B`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(0, 10, (3, 5))\n",
    "B = np.random.randint(0, 10, (5, 2))\n",
    "\n",
    "dot_result = np.dot(A, B)\n",
    "at_result = A @ B\n",
    "\n",
    "print(A.shape, B.shape)\n",
    "print()\n",
    "print(dot_result)\n",
    "print()\n",
    "print(at_result)\n",
    "print()\n",
    "print(np.allclose(dot_result, at_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-45",
   "metadata": {},
   "source": [
    "### Matrices y vectores  \n",
    "\n",
    "También podemos multiplicar matrices por un vector... de nuevo, las reglas anteriormente definidas aplican:  \n",
    "\n",
    "$$\\vec{x} = \n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\dots\\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "El vector de acá arriba se conoce como **vector columna** de dimensión $D \\times 1$, mientras que el de abajo se conoce como **vector fila** de dimensión $1 \\times D$: \n",
    "\n",
    "$$\\vec{y} =\n",
    "\\begin{bmatrix}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\dots & \n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Dada nuestra definición de multiplicación, multiplicar $\\vec{x}\\vec{y}$ resultará en una matriz de $D \\times D$ (el producto externo 😉), mientras que multiplicar $\\vec{y}\\vec{x}$ resultará en un escalar (el producto interno 😉):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1, 2, 3]])\n",
    "y = np.array([[4, 5, 6]])\n",
    "\n",
    "print(\"x\", x.shape)\n",
    "print(x, \"\\n\")\n",
    "print(\"y\", y.shape)\n",
    "print(y, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.outer(x, y))\n",
    "print(x.T @ y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.inner(y, x))\n",
    "print(y @ x.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplicación matricial\n",
    "\n",
    "x = np.array([1, 2, 3])\n",
    "A = np.array([[1, 0, 1], [1, 0, 0], [0, 1, 1]])  # 3x3\n",
    "print(x.shape)\n",
    "print(x)\n",
    "print(A.shape)\n",
    "print(A)\n",
    "print()\n",
    "print(A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "\n",
    "\n",
    "print(np.dot(x, x))\n",
    "print(x @ x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-51",
   "metadata": {},
   "source": [
    " > ⚠️ A diferencia de con los números reales, la mutliplicación de matrices no es conmutativa:\n",
    "        \n",
    "$$AB \\neq BA$$\n",
    "\n",
    " > ⚠️ Aún que las dimensiones sean compatibles, en general al multiplicación de matrices no es conmutativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-52",
   "metadata": {},
   "source": [
    "### Exponenciación de matrices  \n",
    "\n",
    "Una vez que tenemos a nuestra disposición la multiplicación, podemos definir el hecho de multiplicar una matriz **cuadrada** por si misma como la exponenciación matricial:  \n",
    "\n",
    "$$A^4=AAAA$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-53",
   "metadata": {},
   "source": [
    "## Matrices especiales  \n",
    "\n",
    "### Matriz diagonal  \n",
    "\n",
    "Matrices para las que los elementos $A_{ij} == 0$ en donde $i \\neq j$ se llaman matrices diagonales, tienen significado especial en el álgebra lineal.  \n",
    "\n",
    "Podemos convertir un vector de longitud $D$ en una matriz de $D \\times D$ con los elementos del vector en la diagonal usando `np.diag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (4,))\n",
    "diag_x = np.diag(x)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-55",
   "metadata": {},
   "source": [
    "Es posible extraer el vector de la diagonal de una matriz usando... `np.diag`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (4, 4))\n",
    "diag_x = np.diag(x)\n",
    "\n",
    "print(x)\n",
    "print()\n",
    "print(diag_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-57",
   "metadata": {},
   "source": [
    "### Matriz identidad  \n",
    "\n",
    "Una matriz cuadrada cuyos elementos son $0$ en todas partes excepto en la diagonal en donde los valores son $1$ es conocida como una matriz identidad.  \n",
    "\n",
    "La matriz identidad no provoca ningún cambio cuando la usamos en una multiplicación. En *NumPy* podemos usar `np.eye(n)` para generar una matriz identidad de tamaño `n`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0, 10, (3, 3))\n",
    "I = np.eye(3, dtype=int)\n",
    "\n",
    "print(x)\n",
    "print(x @ I)\n",
    "print(I @ x)\n",
    "# print(I * x) # Not the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-59",
   "metadata": {},
   "source": [
    "### Matriz cero  \n",
    "\n",
    "Ah... lo que dice el título"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-60",
   "metadata": {},
   "source": [
    "### Matriz cuadrada  \n",
    "\n",
    "🔲🔳◾️▫️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-61",
   "metadata": {},
   "source": [
    "### Matriz triangular  \n",
    "\n",
    "Una matriz es considerada triangular si los elementos debajo o arriba de la diagonal son cero. \n",
    "\n",
    "Existen dos subtipos:  \n",
    "\n",
    " - **Triangular superior**\n",
    " \n",
    "$$\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "0 & 5 & 6 & 7 \\\\\n",
    "0 & 0 & 8 & 9 \\\\\n",
    "0 & 0 & 0 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    " \n",
    " - **Triangular inferior**  \n",
    " \n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "2 & 3 & 0 & 0 \\\\\n",
    "4 & 5 & 6 & 0 \\\\\n",
    "7 & 8 & 9 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Podemos usar `np.tri(n)` para generar una matriz triangular inferior de $1$ usando *NumPy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05-algebra-lineal-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "triangular = np.tri(4)\n",
    "\n",
    "print(triangular)\n",
    "print()\n",
    "print(triangular.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-63",
   "metadata": {},
   "source": [
    "### Matrices dispersas  \n",
    "\n",
    "A pesar de que no hemos hablado de estas matrices (ni hablaremos en el futuro en este curso) sobre matrices dispersas, son una parte central en la ciencia de datos.  \n",
    "\n",
    "Imagina que en Spotify tienen una matriz en donde las filas son las todas las canciones y las columnas los usuarios, y los valores dentro de la matriz representan si al usuario le gusta o no dicha canción, ¿cómo crees que se vería esa matriz?\n",
    "\n",
    "|                    | Feregrino | Alma      | Benito    | ... | Terry     | Destiny   |\n",
    "|--------------------|-----------|-----------|-----------|-----|-----------|-----------|\n",
    "| La Puerta Negra    | ${\\bf 1}$ | $0$       | $0$       |     | $0$       | $0$       |\n",
    "| Flux               | $0$       | ${\\bf 1}$ | $0$       |     | ${\\bf 1}$ | $0$       |\n",
    "| La mesa del rincón | ${\\bf 1}$ | $0$       | $0$       |     | $0$       | $0$       |\n",
    "| Andar conmigo      | ${\\bf 1}$ | ${\\bf 1}$ | $0$       |     | ${\\bf 1}$ | $0$       |\n",
    "| ...                |           |           |           | ... |           |           |\n",
    "| Dark Horse         | $0$       | ${\\bf 1}$ | ${\\bf 1}$ |     | $0$       | ${\\bf 1}$ |\n",
    "\n",
    "Lo más seguro es que la gran mayoría de las personas no disfruten de la gran mayoría de las canciones, entonces la matriz de acá arriba muy probablemente estará dominada por $0$, con muy pocos $1$ esparcidos aquí y allá.\n",
    "\n",
    "¿Cómo lidiarías con ella?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05-algebra-lineal-64",
   "metadata": {},
   "source": [
    "---------\n",
    "## Resources\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtu.be/bgEML8Re2ks\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "### Sitios web\n",
    "\n",
    " - **Espacios y subespacios vectoriales** - [https://aga.frba.utn.edu.ar/espacios-y-subespacios-vectoriales/](https://aga.frba.utn.edu.ar/espacios-y-subespacios-vectoriales/) \n",
    " \n",
    " - **High-Dimensional Space** - [https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf](https://www.cs.cmu.edu/~venkatg/teaching/CStheory-infoage/chap1-high-dim-space.pdf)\n",
    " \n",
    " - **Maldición de la dimensión** - [https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)  \n",
    " \n",
    " - **List of named matrices** - [https://en.wikipedia.org/wiki/List_of_named_matrices](https://en.wikipedia.org/wiki/List_of_named_matrices)\n",
    "\n",
    "### Videos\n",
    "\n",
    " - **3blue1brown Linear Algebra series** -[https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)  \n",
    "\n",
    "### Libros\n",
    "\n",
    " - **Introduction to applied linear algebra** - [http://stanford.edu/~boyd/vmls/vmls.pdf](http://stanford.edu/~boyd/vmls/vmls.pdf)\n",
    "\n",
    " - **Coding the matrix** - [https://codingthematrix.com/](https://codingthematrix.com/)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
