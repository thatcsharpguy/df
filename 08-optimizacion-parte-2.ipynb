{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce735c4d",
   "metadata": {},
   "source": [
    "# 08: Optimizaci칩n - Parte 2\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://www.youtube.com/watch?v=J4eCD38WF5k\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5fa56e-6bb9-43f8-9c10-6093f5e607a2",
   "metadata": {},
   "source": [
    "## Paquetes  \n",
    "\n",
    " - `numpy`  \n",
    " - `matplotlib`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494b17b",
   "metadata": {},
   "source": [
    "## Piensa en las redes neuronales  \n",
    "\n",
    "Seguramente habr치s escuchado muchas, much칤simas cosas sobre las redes neuronales; no es por nada, sus logros en diversos 치mbitos: visi칩n artificial, procesamiento de lenguaje natural, traducciones, clasificaci칩n de im치genes...  \n",
    "\n",
    "\n",
    "Como ya lo vimos la clase pasada, el problema fundamental de las redes neuronales es el de encontrar la soluci칩n aproximada a una funci칩n. Piensa en un modelo simple, en donde dado un conjunto de observaciones $\\vec{x}_1, \\vec{x}_2, \\dots, \\vec{x}_n$ y un conjunto de valores esperados $y_1, y_2, \\dots, y_n$, tratamos de encontrar una funci칩n $y^\\prime = f(\\vec{x};\\theta)$ y sus par치metros $\\theta$, de tal modo que:\n",
    "\n",
    "$$\\theta^* = \\text{arg min}_{\\theta}  \\sum_i ||f(\\vec{x}_i;\\theta)- y_i||$$\n",
    "\n",
    "En donde $||f(\\vec{x}_i;\\theta)- y_i||$ representa la distancia de la salida de $f$ y los valores esperados para $y_i$.  \n",
    "\n",
    "**Este es un problema de optimizaci칩n**.\n",
    "\n",
    "Sin embargo, las redes neuronales... en particular las pertenencientes al aprendizaje profundo o (*deep learning*), pueden tener decenas, cientos... billones de par치metros; imag칤nate el tama침o de $\\theta$... 쯤u칠 forma de optimizaci칩n emplearias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1aa2d-4922-4a28-a2e9-2a921ee5c826",
   "metadata": {},
   "source": [
    "## Retropropagaci칩n  \n",
    "\n",
    "Antes de explicarte c칩mo es que funciona la retropropagaci칩n, veamos c칩mo es que est치n construidas las redes neuronales, en su forma m치s b치sica:\n",
    "\n",
    "Una serie de capas, cada una de ellas representa una aplicaci칩n lineal (una multiplicaci칩n matricial 游땔) seguida de una funci칩n no lineal. La salida de una capa es la entrada de otra; as칤 hasta llegar a un punto en el cual llegar a un resultado $y^\\prime$:\n",
    "\n",
    "<img src=\"assets/08/BasicNN.png\" />\n",
    "\n",
    "Cada una de las $W_n$ en la imagen anterior representan una matriz distinta, cada una de ellas representa una **matriz de ponderaci칩n** (matriz de pesos o *weight matrix*). En conjunto, todos los valores de estas $W$ forman los par치metros de nuestra red neuronal (nuestro $\\theta$), es decir, los valores que estamos tratando de encontrar. Las funciones $G$ se mantienen \"constantes\".  \n",
    "\n",
    "Esta forma de construir las redes neuronales nos calcular la derivada de la funci칩n objetivo con respecto a cada uno de los par치metros; esta derivada nos ayuda a guiar el camino a seguir para encontrar un m칤nimo de esta funci칩n objetivo. Esto funciona porque la derivada con respecto a los par치metros nos \"dice\" qu칠 tanto efecto tiene cada par치metro en el resultado final.  \n",
    "\n",
    "Los recursos al final del post lo explican mucho, mucho mejor.\n",
    "\n",
    "Esto funciona a칰n cuando hay m칰ltiples capas en nuestra red neuronal, es un algoritmo muy especial conocido como **retropropagaci칩n** o *backpropagation*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc1cb4-c36a-447a-8194-3fbae0a848f2",
   "metadata": {},
   "source": [
    "## 쯇or qu칠 usar este m칠todo?  \n",
    "\n",
    "A diferencia de los m칠todos que vimos la sesi칩n pasada, estos...\n",
    "\n",
    " - Tienden a ser lentos,\n",
    " - No nos garantizan encontrar un punto m칤nimo,\n",
    " - Tienen muchos h칤perparametros que elegir de antemano\n",
    " \n",
    "Cuando se trata de optimizar problemas, como los presentados por las redes neuronales, el usar uno de los m칠todos discutidos antetiormente es ingenuo; en su lugar se usa algo conocido como **optimizaci칩n de primer orden**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fad971-f16e-454a-86c6-38c4d57f4563",
   "metadata": {},
   "source": [
    "## Diferenciabilidad  \n",
    "\n",
    "Ya hab칤amos hablado sobre lo que significa que una funci칩n sea continua. Ahora vamos a hablar de una **funci칩n suave**; una funci칩n de este tipo tiene derivadas continuas hasta cierto orden. Una funci칩n suave es m치s f치cil de optimizar porque esto significa que cambios peque침os en los par치metros provocaran cambios peque침os en el resultado de la funci칩n objetivo.  \n",
    "\n",
    "Si nosotros logramos calcular la derivada exacta de la funci칩n objetivo la optimizaci칩n es mucho, mucho m치s sencilla porque podemos usarla para tomar buenas decisiones sobre hacia d칩nde \"mover\" los par치metros, acerc치ndolos al m칤nimo. Piensa en una funci칩n $L(\\theta) = \\theta^2$ objetivo en donde el vector objetivo est치 formado por un solo valor $\\theta \\in \\mathbb{R}$, \n",
    "\n",
    "$$L(\\theta) = \\theta^2$$  \n",
    "\n",
    "$$L^{\\prime}(\\theta) = 2\\theta$$  \n",
    "\n",
    "La cosa se complica cuando tenemos que lidiar con vectores par치metro de m치s de una entrada, es ah칤 en donde entramos en el terreno de funciones objetivo multidimensionales; en este caso tenemos algo conocido como vector gradiente ($\\nabla L(\\theta)$), en lugar de un solo valor multiplicado por $\\theta$. A칰n as칤, los mismos principios aplican.\n",
    "\n",
    "\n",
    "## Optimizaci칩n con derivadas  \n",
    "\n",
    "Si sabemos (o podemos calcular) el gradiente de una funci칩n objetivo, tambi칠n sabremos la \"pendiente\" de la funci칩n en un punto determinado... esta pendiente nos dicta la direcci칩n hacia la cual la funci칩n crece m치s \"r치pidamente\". Si sabemos la derivada de la funci칩n que estamos tratando de optimizar, podemos incrementar en 칩rdenes de magnitud la eficiencia de nuestra optimizaci칩n.\n",
    "\n",
    "### Primer orden  \n",
    "\n",
    "Hay m칠todos de optimizaci칩n que usan las primeras derivadas como herramienta principal, estos se conocen como **m칠todos de primer orden**. La semana pasada hablamos de **m칠todos de orden cero** en donde lo 칰nico que necesitamos saber es la funci칩n objetivo, y tambi칠n hay m칠todos de segundo orden que requieren la segunda derivada de la funci칩n y se llaman (creativamente) **m칠todos de segundo orden**...\n",
    "\n",
    "La derivada que utilizan estos m칠todos es la derivda de la funci칩n objetivo con respecto los par치metros. Esto encesita que conozcamos los gradientes de la funci칩n, en otras palabras, nosotros podemos aplicar solamente esta funci칩n es: **continua** y **diferenciable**.  \n",
    "\n",
    "En un problema multidimensional, podemos escribir el vector de derivadas de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla L(\\vec{\\theta}) = \\left[ \\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_1},  \n",
    "\\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_2}, \\dots, \\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_n},\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "En donde $\\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_1}$ respresenta el cambio en $L$ en la direcci칩n $\\theta_1$ en el punto $\\vec{\\theta}$.\n",
    "\n",
    "El vector $\\nabla L(\\vec{\\theta})$ es conocido como el **gradiente**; en cualquier punto, este gradiente apunta en la direcci칩n en la que la funci칩n cambia m치s r치pidamenmte y su magnitud nos dice la \"velocidad\" con la que este cambio est치 sucediendo.\n",
    "\n",
    "### El gradiente descendiente  \n",
    "\n",
    "El algoritmo m치s b치sico de primer orden es conocido como el algoritmo del **gradiente descendiente**, es relativamente simple comenzando de una $\\theta^{(0)}$, el valor de $\\theta^{(i+1)}$ estar치 determinado por:  \n",
    "\n",
    "$$\\vec{\\theta^{(i+1)}} = \\vec{\\theta^{(i)}} - \\delta \\nabla L(\\vec{\\theta^{(i)}})$$  \n",
    "\n",
    "Ah칤 se ve una $\\delta$, que en adelante conoceremos como el **tama침o del paso**, este es... s칤 un h칤perparametro; ni aqu칤 nos salvamos de los h칤perparametros.  \n",
    "\n",
    "De forma platicada, el algoritmo es el siguiente:  \n",
    "\n",
    " - Elegimos un punto de inicio $\\theta^{(0)}$  \n",
    " - Repetimos...\n",
    "   - Calculamos la inclinaci칩n en ese punto y para todas las direcciones $v = \\nabla L(\\vec{\\theta^{(i)}})$  \n",
    "   - Damos un peque침o pasito de tama침o $\\delta$ en la direcci칩n que nos indique $v$  \n",
    "   - Despu칠s de ese paso habremos llegado a nuestro nuevo punto de inicio  $\\theta^{(1)}$ \n",
    "   \n",
    "Este m칠todo es un compromiso entre una soluci칩n r치pida y una soluci칩n gen칠rica.  \n",
    "\n",
    "#### Implementaci칩n  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe469c-0a99-4b41-a27f-5b9108098b5e",
   "metadata": {},
   "source": [
    "\n",
    "$$L(\\theta) = \\theta^2$$  \n",
    "\n",
    "$$L^{\\prime}(\\theta) = 2\\theta$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0201071-98c4-4b2f-9c14-eea83ec0a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(theta):\n",
    "    return theta**2\n",
    "\n",
    "def dL(theta):\n",
    "    return 2*theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e902d6-8383-4821-b854-6af10e58cbde",
   "metadata": {},
   "source": [
    " - Elegimos un punto de inicio $\\theta^{(0)}$  \n",
    " - Repetimos...\n",
    "   - Calculamos la inclinaci칩n en ese punto y para todas las direcciones $v = \\nabla L(\\vec{\\theta^{(i)}})$  \n",
    "   - Damos un peque침o pasito de tama침o $\\delta$ en la direcci칩n que nos indique $v$  \n",
    "   - Despu칠s de ese paso habremos llegado a nuestro nuevo punto de inicio  $\\theta^{(1)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a63310-3dec-4752-b58a-7efc022590cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerancia = 1e-4\n",
    "\n",
    "def gradiente(loss_fn, derivada_loss_fn, theta_inicial, paso):\n",
    "    theta = theta_inicial\n",
    "    \n",
    "    current_loss = loss_fn(theta)\n",
    "    last_loss = np.inf\n",
    "    historial = [(theta, current_loss)]\n",
    "    \n",
    "    while np.abs(current_loss-last_loss) > tolerancia and len(historial) < 100000:\n",
    "        last_loss = current_loss\n",
    "        \n",
    "        theta = theta - (paso * derivada_loss_fn(theta))\n",
    "        \n",
    "        current_loss = loss_fn(theta)\n",
    "        historial.append((theta, current_loss))\n",
    "    history = np.array(historial)\n",
    "    return history[:,0], history[:,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b969cd1-1908-496c-90c8-071971de4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent(xs, L, dL, x_0, paso, ax):\n",
    "    fig = plt.figure(figsize=(7, 3), dpi=200)\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    x, y  = gradiente(L, dL, x_0, paso)\n",
    "    \n",
    "    ax.fill_between(xs, L(xs), label=\"Funci칩n objetivo\")    \n",
    "    ax.set_title(f\"Paso {paso} - M칤nimo en {len(x)} pasos\")\n",
    "    ax.set_xlabel(\"$\\\\theta$\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.plot(x, y, 'k-x', label='Pasos')\n",
    "    ax.legend()\n",
    "\n",
    "xs = np.linspace(-5,5,200)\n",
    "\n",
    "step_sizes = [0.0001, 0.01, 0.1, 0.3, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e8b30-80a9-4bc5-b757-571a2fd68554",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.0001, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95d4c0-45ef-4163-b0b4-a2ec4a9f39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.001, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda3bcb-df56-43af-b1e7-b15d95b108b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.01, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9052122-aa1b-4b66-983b-e297782dd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.1, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefba77-71ed-45bd-b8a0-2b114ad9d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.99, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3591d-c059-4ad7-b6ff-48287b9c4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 1.000002, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b72fe-a584-44c7-868c-f232c4023918",
   "metadata": {},
   "source": [
    "#### Los \"peros\" del gradiente descendiente\n",
    "\n",
    " - Debemos poder calcular el gradiente de la funci칩n ($L^{\\prime}(\\theta)$) en cualquier punto.  \n",
    " - La velocidad (y convergencia) de la optimizaci칩n depende del tama침o de paso elegido.   \n",
    " - Funciona mejor en funciones \"suaves\" sin variaciones grandes.  \n",
    " - A칰n se puede atorar en m칤nimos locales o \"[puntos de silla](https://es.wikipedia.org/wiki/Punto_de_silla)\".  \n",
    " - Debemos seguir evaluando la funci칩n a cada paso, si esta funci칩n es costosa, la optimizaci칩n es costosa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a04e7-e9ce-46c8-9aa5-f4bbb1d30788",
   "metadata": {},
   "source": [
    "### El gradiente descendiente estoc치stico  \n",
    "\n",
    "De nuestro 칰ltimo punto, evaluar la funci칩n puede ser muy *\"costoso\"*, en particular cuando tenemos datasets grandes (ehem... *machine learning*) o una funci칩n con muchos par치metros (ehem... *machine learning*).\n",
    "\n",
    "Si logramos separar la funci칩n en sub partes, podemos configurar el optimizador para optimizar una parcialidad de estas independientemente, agregando estas *\"peque침as\"* optimizaciones. Este es el principio detr치s detr치s del  algoritmo del **gradiente descendiente estoc치stico**.   \n",
    "\n",
    "Si la funci칩n de p칠rdida se puede escribir de la siguiente manera:   \n",
    "\n",
    "$$L(\\theta) = \\sum_i L_i(\\theta)$$\n",
    "\n",
    "Es decir, si la funci칩n se puede expresar como una suma $L_1(\\theta), L_2(\\theta), \\dots, L_n(\\theta)$.  \n",
    "\n",
    "Para nuestra suerte, cuando estamos tratando de entrenar un modelo de *machine learning* podemos descomponer la funci칩n de p칠rdida de esta manera; y es que en este tipo de problemas tenemos diversos ejemplos de entrenamiento $\\vec{x}_i$ y su correspondiente $y_i$. En este caso, nuestra funci칩n de p칠rdida est치 dada por:  \n",
    "\n",
    "\n",
    "$$L(\\theta) = \\sum_i \\|f(\\vec{x}_i;\\vec{\\theta}) - y_i\\|$$  \n",
    "\n",
    "(una suma sobre todos los ejemplos de entrenamiento)\n",
    "\n",
    "Cuando hablamos de las diferenciales:\n",
    "\n",
    "$$\\nabla \\sum_i \\|f(\\vec{x}_i;\\vec{\\theta}) - y_i\\| = \\sum_i \\nabla ||f(\\vec{x}_i;\\vec{\\theta}) - y_i||$$  \n",
    "\n",
    "Propuesto de esta manera, podemos tomar un subconjunto de ejemplos de entrenamiento y sus salidas, computar el gradiente para este subconjunto y dar el paso; conforme \"entrenamos\" m치s y m치s, en teor칤a, nos iremos acercando al m칤nimo.  \n",
    "\n",
    "A cada uno de estos subconjuntos se les conoce como **minibatch**. Cuando un algoritmo ha visto ya todos los ejemplos de un conjunto de entrenamiento se le conoce como **epoch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a61e5",
   "metadata": {},
   "source": [
    "---------\n",
    "## Recursos\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://www.youtube.com/watch?v=J4eCD38WF5k\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "- **Neural Networks and Deep Learning** - [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/) \n",
    " \n",
    " - **Calculus on Computational Graphs: Backpropagation** - [http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)\n",
    " \n",
    " - **Neural Networks, Manifolds, and Topology** - [http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    " \n",
    " - **Ecuaci칩n diferencial ordinaria de primer orden** - [https://es.wikipedia.org/wiki/Ecuaci%C3%B3n_diferencial_ordinaria_de_primer_orden](https://es.wikipedia.org/wiki/Ecuaci%C3%B3n_diferencial_ordinaria_de_primer_orden)\n",
    " \n",
    " - **smooth functions or continuous** - [https://math.stackexchange.com/questions/472148/smooth-functions-or-continuous](https://math.stackexchange.com/questions/472148/smooth-functions-or-continuous)\n",
    " \n",
    " - **does it make sense to talk about third-order (or higher order) optimization methods?** - [https://math.stackexchange.com/questions/2838083/does-it-make-sense-to-talk-about-third-order-or-higher-order-optimization-meth](https://math.stackexchange.com/questions/2838083/does-it-make-sense-to-talk-about-third-order-or-higher-order-optimization-meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd89794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
