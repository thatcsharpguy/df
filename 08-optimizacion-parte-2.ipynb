{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce735c4d",
   "metadata": {},
   "source": [
    "# 08: Optimización - Parte 2\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://www.youtube.com/watch?v=J4eCD38WF5k\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5fa56e-6bb9-43f8-9c10-6093f5e607a2",
   "metadata": {},
   "source": [
    "## Paquetes  \n",
    "\n",
    " - `numpy`  \n",
    " - `matplotlib`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494b17b",
   "metadata": {},
   "source": [
    "## Piensa en las redes neuronales  \n",
    "\n",
    "Seguramente habrás escuchado muchas, muchísimas cosas sobre las redes neuronales; no es por nada, sus logros en diversos ámbitos: visión artificial, procesamiento de lenguaje natural, traducciones, clasificación de imágenes...  \n",
    "\n",
    "\n",
    "Como ya lo vimos la clase pasada, el problema fundamental de las redes neuronales es el de encontrar la solución aproximada a una función. Piensa en un modelo simple, en donde dado un conjunto de observaciones $\\vec{x}_1, \\vec{x}_2, \\dots, \\vec{x}_n$ y un conjunto de valores esperados $y_1, y_2, \\dots, y_n$, tratamos de encontrar una función $y^\\prime = f(\\vec{x};\\theta)$ y sus parámetros $\\theta$, de tal modo que:\n",
    "\n",
    "$$\\theta^* = \\text{arg min}_{\\theta}  \\sum_i ||f(\\vec{x}_i;\\theta)- y_i||$$\n",
    "\n",
    "En donde $||f(\\vec{x}_i;\\theta)- y_i||$ representa la distancia de la salida de $f$ y los valores esperados para $y_i$.  \n",
    "\n",
    "**Este es un problema de optimización**.\n",
    "\n",
    "Sin embargo, las redes neuronales... en particular las pertenencientes al aprendizaje profundo o (*deep learning*), pueden tener decenas, cientos... billones de parámetros; imagínate el tamaño de $\\theta$... ¿qué forma de optimización emplearias?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1aa2d-4922-4a28-a2e9-2a921ee5c826",
   "metadata": {},
   "source": [
    "## Retropropagación  \n",
    "\n",
    "Antes de explicarte cómo es que funciona la retropropagación, veamos cómo es que están construidas las redes neuronales, en su forma más básica:\n",
    "\n",
    "Una serie de capas, cada una de ellas representa una aplicación lineal (una multiplicación matricial 😉) seguida de una función no lineal. La salida de una capa es la entrada de otra; así hasta llegar a un punto en el cual llegar a un resultado $y^\\prime$:\n",
    "\n",
    "<img src=\"assets/08/BasicNN.png\" />\n",
    "\n",
    "Cada una de las $W_n$ en la imagen anterior representan una matriz distinta, cada una de ellas representa una **matriz de ponderación** (matriz de pesos o *weight matrix*). En conjunto, todos los valores de estas $W$ forman los parámetros de nuestra red neuronal (nuestro $\\theta$), es decir, los valores que estamos tratando de encontrar. Las funciones $G$ se mantienen \"constantes\".  \n",
    "\n",
    "Esta forma de construir las redes neuronales nos calcular la derivada de la función objetivo con respecto a cada uno de los parámetros; esta derivada nos ayuda a guiar el camino a seguir para encontrar un mínimo de esta función objetivo. Esto funciona porque la derivada con respecto a los parámetros nos \"dice\" qué tanto efecto tiene cada parámetro en el resultado final.  \n",
    "\n",
    "Los recursos al final del post lo explican mucho, mucho mejor.\n",
    "\n",
    "Esto funciona aún cuando hay múltiples capas en nuestra red neuronal, es un algoritmo muy especial conocido como **retropropagación** o *backpropagation*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc1cb4-c36a-447a-8194-3fbae0a848f2",
   "metadata": {},
   "source": [
    "## ¿Por qué usar este método?  \n",
    "\n",
    "A diferencia de los métodos que vimos la sesión pasada, estos...\n",
    "\n",
    " - Tienden a ser lentos,\n",
    " - No nos garantizan encontrar un punto mínimo,\n",
    " - Tienen muchos híperparametros que elegir de antemano\n",
    " \n",
    "Cuando se trata de optimizar problemas, como los presentados por las redes neuronales, el usar uno de los métodos discutidos antetiormente es ingenuo; en su lugar se usa algo conocido como **optimización de primer orden**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fad971-f16e-454a-86c6-38c4d57f4563",
   "metadata": {},
   "source": [
    "## Diferenciabilidad  \n",
    "\n",
    "Ya habíamos hablado sobre lo que significa que una función sea continua. Ahora vamos a hablar de una **función suave**; una función de este tipo tiene derivadas continuas hasta cierto orden. Una función suave es más fácil de optimizar porque esto significa que cambios pequeños en los parámetros provocaran cambios pequeños en el resultado de la función objetivo.  \n",
    "\n",
    "Si nosotros logramos calcular la derivada exacta de la función objetivo la optimización es mucho, mucho más sencilla porque podemos usarla para tomar buenas decisiones sobre hacia dónde \"mover\" los parámetros, acercándolos al mínimo. Piensa en una función $L(\\theta) = \\theta^2$ objetivo en donde el vector objetivo está formado por un solo valor $\\theta \\in \\mathbb{R}$, \n",
    "\n",
    "$$L(\\theta) = \\theta^2$$  \n",
    "\n",
    "$$L^{\\prime}(\\theta) = 2\\theta$$  \n",
    "\n",
    "La cosa se complica cuando tenemos que lidiar con vectores parámetro de más de una entrada, es ahí en donde entramos en el terreno de funciones objetivo multidimensionales; en este caso tenemos algo conocido como vector gradiente ($\\nabla L(\\theta)$), en lugar de un solo valor multiplicado por $\\theta$. Aún así, los mismos principios aplican.\n",
    "\n",
    "\n",
    "## Optimización con derivadas  \n",
    "\n",
    "Si sabemos (o podemos calcular) el gradiente de una función objetivo, también sabremos la \"pendiente\" de la función en un punto determinado... esta pendiente nos dicta la dirección hacia la cual la función crece más \"rápidamente\". Si sabemos la derivada de la función que estamos tratando de optimizar, podemos incrementar en órdenes de magnitud la eficiencia de nuestra optimización.\n",
    "\n",
    "### Primer orden  \n",
    "\n",
    "Hay métodos de optimización que usan las primeras derivadas como herramienta principal, estos se conocen como **métodos de primer orden**. La semana pasada hablamos de **métodos de orden cero** en donde lo único que necesitamos saber es la función objetivo, y también hay métodos de segundo orden que requieren la segunda derivada de la función y se llaman (creativamente) **métodos de segundo orden**...\n",
    "\n",
    "La derivada que utilizan estos métodos es la derivda de la función objetivo con respecto los parámetros. Esto encesita que conozcamos los gradientes de la función, en otras palabras, nosotros podemos aplicar solamente esta función es: **continua** y **diferenciable**.  \n",
    "\n",
    "En un problema multidimensional, podemos escribir el vector de derivadas de la siguiente manera:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\nabla L(\\vec{\\theta}) = \\left[ \\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_1},  \n",
    "\\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_2}, \\dots, \\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_n},\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "En donde $\\frac{\\partial L(\\vec{\\theta})}{\\partial \\theta_1}$ respresenta el cambio en $L$ en la dirección $\\theta_1$ en el punto $\\vec{\\theta}$.\n",
    "\n",
    "El vector $\\nabla L(\\vec{\\theta})$ es conocido como el **gradiente**; en cualquier punto, este gradiente apunta en la dirección en la que la función cambia más rápidamenmte y su magnitud nos dice la \"velocidad\" con la que este cambio está sucediendo.\n",
    "\n",
    "### El gradiente descendiente  \n",
    "\n",
    "El algoritmo más básico de primer orden es conocido como el algoritmo del **gradiente descendiente**, es relativamente simple comenzando de una $\\theta^{(0)}$, el valor de $\\theta^{(i+1)}$ estará determinado por:  \n",
    "\n",
    "$$\\vec{\\theta^{(i+1)}} = \\vec{\\theta^{(i)}} - \\delta \\nabla L(\\vec{\\theta^{(i)}})$$  \n",
    "\n",
    "Ahí se ve una $\\delta$, que en adelante conoceremos como el **tamaño del paso**, este es... sí un híperparametro; ni aquí nos salvamos de los híperparametros.  \n",
    "\n",
    "De forma platicada, el algoritmo es el siguiente:  \n",
    "\n",
    " - Elegimos un punto de inicio $\\theta^{(0)}$  \n",
    " - Repetimos...\n",
    "   - Calculamos la inclinación en ese punto y para todas las direcciones $v = \\nabla L(\\vec{\\theta^{(i)}})$  \n",
    "   - Damos un pequeño pasito de tamaño $\\delta$ en la dirección que nos indique $v$  \n",
    "   - Después de ese paso habremos llegado a nuestro nuevo punto de inicio  $\\theta^{(1)}$ \n",
    "   \n",
    "Este método es un compromiso entre una solución rápida y una solución genérica.  \n",
    "\n",
    "#### Implementación  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe469c-0a99-4b41-a27f-5b9108098b5e",
   "metadata": {},
   "source": [
    "\n",
    "$$L(\\theta) = \\theta^2$$  \n",
    "\n",
    "$$L^{\\prime}(\\theta) = 2\\theta$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0201071-98c4-4b2f-9c14-eea83ec0a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L(theta):\n",
    "    return theta**2\n",
    "\n",
    "def dL(theta):\n",
    "    return 2*theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e902d6-8383-4821-b854-6af10e58cbde",
   "metadata": {},
   "source": [
    " - Elegimos un punto de inicio $\\theta^{(0)}$  \n",
    " - Repetimos...\n",
    "   - Calculamos la inclinación en ese punto y para todas las direcciones $v = \\nabla L(\\vec{\\theta^{(i)}})$  \n",
    "   - Damos un pequeño pasito de tamaño $\\delta$ en la dirección que nos indique $v$  \n",
    "   - Después de ese paso habremos llegado a nuestro nuevo punto de inicio  $\\theta^{(1)}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a63310-3dec-4752-b58a-7efc022590cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tolerancia = 1e-4\n",
    "\n",
    "def gradiente(loss_fn, derivada_loss_fn, theta_inicial, paso):\n",
    "    theta = theta_inicial\n",
    "    \n",
    "    current_loss = loss_fn(theta)\n",
    "    last_loss = np.inf\n",
    "    historial = [(theta, current_loss)]\n",
    "    \n",
    "    while np.abs(current_loss-last_loss) > tolerancia and len(historial) < 100000:\n",
    "        last_loss = current_loss\n",
    "        \n",
    "        theta = theta - (paso * derivada_loss_fn(theta))\n",
    "        \n",
    "        current_loss = loss_fn(theta)\n",
    "        historial.append((theta, current_loss))\n",
    "    history = np.array(historial)\n",
    "    return history[:,0], history[:,1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b969cd1-1908-496c-90c8-071971de4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient_descent(xs, L, dL, x_0, paso, ax):\n",
    "    fig = plt.figure(figsize=(7, 3), dpi=200)\n",
    "    ax = fig.gca()\n",
    "    \n",
    "    x, y  = gradiente(L, dL, x_0, paso)\n",
    "    \n",
    "    ax.fill_between(xs, L(xs), label=\"Función objetivo\")    \n",
    "    ax.set_title(f\"Paso {paso} - Mínimo en {len(x)} pasos\")\n",
    "    ax.set_xlabel(\"$\\\\theta$\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.plot(x, y, 'k-x', label='Pasos')\n",
    "    ax.legend()\n",
    "\n",
    "xs = np.linspace(-5,5,200)\n",
    "\n",
    "step_sizes = [0.0001, 0.01, 0.1, 0.3, 0.99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e8b30-80a9-4bc5-b757-571a2fd68554",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.0001, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d95d4c0-45ef-4163-b0b4-a2ec4a9f39dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.001, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cda3bcb-df56-43af-b1e7-b15d95b108b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.01, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9052122-aa1b-4b66-983b-e297782dd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.1, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eefba77-71ed-45bd-b8a0-2b114ad9d180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 0.99, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3591d-c059-4ad7-b6ff-48287b9c4c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_descent(xs, L, dL, -5, 1.000002, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592b72fe-a584-44c7-868c-f232c4023918",
   "metadata": {},
   "source": [
    "#### Los \"peros\" del gradiente descendiente\n",
    "\n",
    " - Debemos poder calcular el gradiente de la función ($L^{\\prime}(\\theta)$) en cualquier punto.  \n",
    " - La velocidad (y convergencia) de la optimización depende del tamaño de paso elegido.   \n",
    " - Funciona mejor en funciones \"suaves\" sin variaciones grandes.  \n",
    " - Aún se puede atorar en mínimos locales o \"[puntos de silla](https://es.wikipedia.org/wiki/Punto_de_silla)\".  \n",
    " - Debemos seguir evaluando la función a cada paso, si esta función es costosa, la optimización es costosa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465a04e7-e9ce-46c8-9aa5-f4bbb1d30788",
   "metadata": {},
   "source": [
    "### El gradiente descendiente estocástico  \n",
    "\n",
    "De nuestro último punto, evaluar la función puede ser muy *\"costoso\"*, en particular cuando tenemos datasets grandes (ehem... *machine learning*) o una función con muchos parámetros (ehem... *machine learning*).\n",
    "\n",
    "Si logramos separar la función en sub partes, podemos configurar el optimizador para optimizar una parcialidad de estas independientemente, agregando estas *\"pequeñas\"* optimizaciones. Este es el principio detrás detrás del  algoritmo del **gradiente descendiente estocástico**.   \n",
    "\n",
    "Si la función de pérdida se puede escribir de la siguiente manera:   \n",
    "\n",
    "$$L(\\theta) = \\sum_i L_i(\\theta)$$\n",
    "\n",
    "Es decir, si la función se puede expresar como una suma $L_1(\\theta), L_2(\\theta), \\dots, L_n(\\theta)$.  \n",
    "\n",
    "Para nuestra suerte, cuando estamos tratando de entrenar un modelo de *machine learning* podemos descomponer la función de pérdida de esta manera; y es que en este tipo de problemas tenemos diversos ejemplos de entrenamiento $\\vec{x}_i$ y su correspondiente $y_i$. En este caso, nuestra función de pérdida está dada por:  \n",
    "\n",
    "\n",
    "$$L(\\theta) = \\sum_i \\|f(\\vec{x}_i;\\vec{\\theta}) - y_i\\|$$  \n",
    "\n",
    "(una suma sobre todos los ejemplos de entrenamiento)\n",
    "\n",
    "Cuando hablamos de las diferenciales:\n",
    "\n",
    "$$\\nabla \\sum_i \\|f(\\vec{x}_i;\\vec{\\theta}) - y_i\\| = \\sum_i \\nabla ||f(\\vec{x}_i;\\vec{\\theta}) - y_i||$$  \n",
    "\n",
    "Propuesto de esta manera, podemos tomar un subconjunto de ejemplos de entrenamiento y sus salidas, computar el gradiente para este subconjunto y dar el paso; conforme \"entrenamos\" más y más, en teoría, nos iremos acercando al mínimo.  \n",
    "\n",
    "A cada uno de estos subconjuntos se les conoce como **minibatch**. Cuando un algoritmo ha visto ya todos los ejemplos de un conjunto de entrenamiento se le conoce como **epoch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7a61e5",
   "metadata": {},
   "source": [
    "---------\n",
    "## Recursos\n",
    "\n",
    "<table style=\"margin:0; max-width: 1000px;\">\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>\n",
    "                <a href=\"https://thatcsharpguy.com\">\n",
    "                    <img src=\"assets/general/Sharp@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitter.com/io_exception\">\n",
    "                    <img src=\"assets/general/Twitter@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://tcsg.dev/discord\">\n",
    "                    <img src=\"assets/general/Discord@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://github.com/thatcsharpguy/df\">\n",
    "                    <img src=\"assets/general/GitHub@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://youtube.com/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/YouTube@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://www.youtube.com/watch?v=J4eCD38WF5k\">\n",
    "                    <img src=\"assets/general/EnVivo@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "            <td>\n",
    "                <a href=\"https://twitch.tv/thatcsharpguy\">\n",
    "                    <img src=\"assets/general/Twitch@1x.png\" />\n",
    "                </a>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "- **Neural Networks and Deep Learning** - [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/) \n",
    " \n",
    " - **Calculus on Computational Graphs: Backpropagation** - [http://colah.github.io/posts/2015-08-Backprop/](http://colah.github.io/posts/2015-08-Backprop/)\n",
    " \n",
    " - **Neural Networks, Manifolds, and Topology** - [http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)\n",
    " \n",
    " - **Ecuación diferencial ordinaria de primer orden** - [https://es.wikipedia.org/wiki/Ecuaci%C3%B3n_diferencial_ordinaria_de_primer_orden](https://es.wikipedia.org/wiki/Ecuaci%C3%B3n_diferencial_ordinaria_de_primer_orden)\n",
    " \n",
    " - **smooth functions or continuous** - [https://math.stackexchange.com/questions/472148/smooth-functions-or-continuous](https://math.stackexchange.com/questions/472148/smooth-functions-or-continuous)\n",
    " \n",
    " - **does it make sense to talk about third-order (or higher order) optimization methods?** - [https://math.stackexchange.com/questions/2838083/does-it-make-sense-to-talk-about-third-order-or-higher-order-optimization-meth](https://math.stackexchange.com/questions/2838083/does-it-make-sense-to-talk-about-third-order-or-higher-order-optimization-meth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd89794",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
